{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02a â€” LLM Offline Evaluation (core)\n",
        "\n",
        "Timebox: 45â€“60 minutes\n",
        "\n",
        "Compute EM/F1, simple ROUGE-lite, and rubric-based scoring over toy pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from utils.datasets import read_csv, data_path\n",
        "from utils.metrics import exact_match, f1_score, rouge_lite\n",
        "from utils.grading import run_checks, assert_equal, assert_true\n",
        "\n",
        "pairs = read_csv(\"toy/toy_text_pairs.csv\")\n",
        "\n",
        "pairs[\"em\"] = [exact_match(p, r) for p, r in zip(pairs[\"prediction\"], pairs[\"reference\"])]\n",
        "pairs[\"f1\"] = [f1_score(p, r) for p, r in zip(pairs[\"prediction\"], pairs[\"reference\"])]\n",
        "pairs[[\"rouge_p\",\"rouge_r\"]] = [\n",
        "    pd.Series(rouge_lite(p, r)) for p, r in zip(pairs[\"prediction\"], pairs[\"reference\"])\n",
        "]\n",
        "\n",
        "# rubric demo: treat correctness as EM>0 or f1 threshold\n",
        "pairs[\"rubric_correctness\"] = (pairs[\"em\"] | (pairs[\"f1\"] > 0.6)).astype(int) * 5\n",
        "\n",
        "em_mean = float(pairs[\"em\"].mean())\n",
        "f1_mean = float(pairs[\"f1\"].mean())\n",
        "\n",
        "chk1 = lambda: assert_true(\"has rows\", len(pairs) >= 3)\n",
        "chk2 = lambda: assert_true(\"em range\", 0.0 <= em_mean <= 1.0)\n",
        "chk3 = lambda: assert_true(\"f1 range\", 0.0 <= f1_mean <= 1.0)\n",
        "run_checks(chk1, chk2, chk3)\n",
        "\n",
        "pairs.head()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
