"""
Automated Test Runner for Medium Data/ML Coding Exercises

This script tests all your implementations and compares them against reference solutions.

Run with: python test_all.py
Or test individual exercises: python test_all.py --exercise 1
"""

import sys
import os
import time
import traceback
from pathlib import Path
from typing import Any, Callable, List, Tuple
import importlib.util
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from datetime import datetime

# Get the directory where this script is located
SCRIPT_DIR = Path(__file__).parent.absolute()

# ANSI color codes for terminal output
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    RESET = '\033[0m'
    BOLD = '\033[1m'


def load_module(file_path: str, module_name: str):
    """Dynamically load a Python module from a file path."""
    full_path = SCRIPT_DIR / file_path
    if not full_path.exists():
        raise FileNotFoundError(f"Could not find {full_path}")
    
    spec = importlib.util.spec_from_file_location(module_name, str(full_path))
    if spec is None or spec.loader is None:
        raise ImportError(f"Could not load {full_path}")
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


def test_exercise_1():
    """Test Exercise 1: Model Versioning and Registry"""
    print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*60}")
    print("Exercise 1: Model Versioning and Registry")
    print(f"{'='*60}{Colors.RESET}")
    
    try:
        student_module = load_module('starter_01.py', 'starter_01')
        solution_module = load_module('solution_01.py', 'solution_01')
        
        passed = 0
        failed = 0
        
        # Test student implementation
        try:
            # Clean up any existing registry
            test_registry_path = SCRIPT_DIR / "test_model_registry"
            if test_registry_path.exists():
                import shutil
                shutil.rmtree(test_registry_path)
            
            student_registry = student_module.ModelRegistry(test_registry_path)
            
            # Train a simple model
            X, y = make_classification(n_samples=50, n_features=5, random_state=42)
            model = RandomForestClassifier(random_state=42)
            model.fit(X, y)
            
            # Register model
            metadata = student_module.ModelMetadata(
                version="1.0.0",
                stage=student_module.ModelStage.DEV,
                trained_at=datetime.now(),
                metrics={"accuracy": 0.95},
                features=[f"feature_{i}" for i in range(5)],
                model_type="RandomForest",
                hyperparameters={"n_estimators": 100}
            )
            
            version = student_registry.register_model(model, metadata)
            
            print(f"\n{Colors.BLUE}Student Solution:{Colors.RESET}")
            print(f"  Registered version: {version}")
            
            # Test retrieval
            retrieved_model, retrieved_meta = student_registry.get_model(version=version)
            print(f"  Retrieved model version: {retrieved_meta.version}")
            
            # Test promotion
            student_registry.promote_model(version, student_module.ModelStage.PRODUCTION)
            prod_model, prod_meta = student_registry.get_model(stage=student_module.ModelStage.PRODUCTION)
            print(f"  Production model version: {prod_meta.version}")
            
            if prod_meta.stage == student_module.ModelStage.PRODUCTION:
                print(f"{Colors.GREEN}âœ“ Model promotion works{Colors.RESET}")
                passed += 1
            else:
                print(f"{Colors.RED}âœ— Model promotion failed{Colors.RESET}")
                failed += 1
            
            # Cleanup
            if test_registry_path.exists():
                import shutil
                shutil.rmtree(test_registry_path)
                
        except Exception as e:
            print(f"{Colors.RED}âœ— Student solution: ERROR{Colors.RESET}")
            print(f"  {str(e)}")
            traceback.print_exc()
            failed += 1
        
        print(f"\n{Colors.BOLD}Summary:{Colors.RESET}")
        print(f"{Colors.GREEN}Passed: {passed}{Colors.RESET}")
        print(f"{Colors.RED}Failed: {failed}{Colors.RESET}")
        return passed, failed
        
    except Exception as e:
        print(f"{Colors.RED}Error: {str(e)}{Colors.RESET}")
        return 0, 1


def test_exercise_2():
    """Test Exercise 2: Model Monitoring and Drift Detection"""
    print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*60}")
    print("Exercise 2: Model Monitoring and Drift Detection")
    print(f"{'='*60}{Colors.RESET}")
    
    try:
        student_module = load_module('starter_02.py', 'starter_02')
        solution_module = load_module('solution_02.py', 'solution_02')
        
        passed = 0
        failed = 0
        
        # Generate baseline data
        baseline = pd.DataFrame({
            'feature1': np.random.normal(0, 1, 1000),
            'feature2': np.random.normal(5, 2, 1000),
            'category': np.random.choice(['A', 'B', 'C'], 1000)
        })
        
        baseline_preds = np.random.normal(0.5, 0.1, 1000)
        
        # Test student implementation
        try:
            student_monitor = student_module.ModelMonitor(baseline, baseline_preds)
            
            # Simulate drift
            current = pd.DataFrame({
                'feature1': np.random.normal(2, 1, 100),  # Drifted!
                'feature2': np.random.normal(5, 2, 100),
                'category': np.random.choice(['A', 'B', 'C'], 100)
            })
            
            alerts = student_monitor.detect_data_drift(current)
            
            print(f"\n{Colors.BLUE}Student Solution:{Colors.RESET}")
            print(f"  Detected {len(alerts)} drift alerts")
            
            if len(alerts) > 0:
                print(f"{Colors.GREEN}âœ“ Drift detection works{Colors.RESET}")
                passed += 1
            else:
                print(f"{Colors.YELLOW}âš  No drift detected (may be expected){Colors.RESET}")
                passed += 1  # Still pass if no false positives
                
        except Exception as e:
            print(f"{Colors.RED}âœ— Student solution: ERROR{Colors.RESET}")
            print(f"  {str(e)}")
            traceback.print_exc()
            failed += 1
        
        print(f"\n{Colors.BOLD}Summary:{Colors.RESET}")
        print(f"{Colors.GREEN}Passed: {passed}{Colors.RESET}")
        print(f"{Colors.RED}Failed: {failed}{Colors.RESET}")
        return passed, failed
        
    except Exception as e:
        print(f"{Colors.RED}Error: {str(e)}{Colors.RESET}")
        return 0, 1


def test_exercise_3():
    """Test Exercise 3: A/B Testing Framework"""
    print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*60}")
    print("Exercise 3: A/B Testing Framework")
    print(f"{'='*60}{Colors.RESET}")
    
    try:
        student_module = load_module('starter_03.py', 'starter_03')
        solution_module = load_module('solution_03.py', 'solution_03')
        
        passed = 0
        failed = 0
        
        # Test student implementation
        try:
            config = student_module.ExperimentConfig(
                variant_a_name="current_model",
                variant_b_name="improved_model",
                traffic_split=0.5,
                minimum_sample_size=100,
                primary_metric="accuracy"
            )
            
            framework = student_module.ABTestFramework(config)
            
            # Simulate experiment
            np.random.seed(42)
            for i in range(200):
                variant = framework.assign_variant()
                if variant == 'A':
                    metrics = {'accuracy': np.random.normal(0.85, 0.02)}
                else:
                    metrics = {'accuracy': np.random.normal(0.87, 0.02)}
                framework.record_result(variant, metrics)
            
            result = framework.analyze_results()
            
            print(f"\n{Colors.BLUE}Student Solution:{Colors.RESET}")
            print(f"  Recommendation: {result.recommendation}")
            print(f"  Sample sizes: A={result.sample_size_a}, B={result.sample_size_b}")
            
            if result.sample_size_a > 0 and result.sample_size_b > 0:
                print(f"{Colors.GREEN}âœ“ A/B testing framework works{Colors.RESET}")
                passed += 1
            else:
                print(f"{Colors.RED}âœ— A/B testing framework failed{Colors.RESET}")
                failed += 1
                
        except Exception as e:
            print(f"{Colors.RED}âœ— Student solution: ERROR{Colors.RESET}")
            print(f"  {str(e)}")
            traceback.print_exc()
            failed += 1
        
        print(f"\n{Colors.BOLD}Summary:{Colors.RESET}")
        print(f"{Colors.GREEN}Passed: {passed}{Colors.RESET}")
        print(f"{Colors.RED}Failed: {failed}{Colors.RESET}")
        return passed, failed
        
    except Exception as e:
        print(f"{Colors.RED}Error: {str(e)}{Colors.RESET}")
        return 0, 1


def test_exercise_4():
    """Test Exercise 4: ML Experiment Tracking"""
    print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*60}")
    print("Exercise 4: ML Experiment Tracking")
    print(f"{'='*60}{Colors.RESET}")
    
    try:
        student_module = load_module('starter_04.py', 'starter_04')
        solution_module = load_module('solution_04.py', 'solution_04')
        
        passed = 0
        failed = 0
        
        # Test student implementation
        try:
            test_tracking_dir = SCRIPT_DIR / "test_experiments"
            if test_tracking_dir.exists():
                import shutil
                shutil.rmtree(test_tracking_dir)
            
            tracker = student_module.ExperimentTracker(test_tracking_dir)
            
            # Start experiment
            run_id = tracker.start_run("test_experiment", tags={"model": "test"})
            
            # Log parameters
            tracker.log_params({
                'n_estimators': 100,
                'max_depth': 10
            })
            
            # Log metrics
            for epoch in range(5):
                tracker.log_metrics({
                    'accuracy': 0.85 + epoch * 0.01,
                    'loss': 0.5 - epoch * 0.05
                }, step=epoch)
            
            # End run
            tracker.end_run()
            
            # Search runs
            runs = tracker.search_runs(experiment_name="test_experiment")
            
            print(f"\n{Colors.BLUE}Student Solution:{Colors.RESET}")
            print(f"  Run ID: {run_id}")
            print(f"  Found {len(runs)} runs")
            
            if len(runs) > 0:
                print(f"{Colors.GREEN}âœ“ Experiment tracking works{Colors.RESET}")
                passed += 1
            else:
                print(f"{Colors.RED}âœ— Experiment tracking failed{Colors.RESET}")
                failed += 1
            
            # Cleanup
            if test_tracking_dir.exists():
                import shutil
                shutil.rmtree(test_tracking_dir)
                
        except Exception as e:
            print(f"{Colors.RED}âœ— Student solution: ERROR{Colors.RESET}")
            print(f"  {str(e)}")
            traceback.print_exc()
            failed += 1
        
        print(f"\n{Colors.BOLD}Summary:{Colors.RESET}")
        print(f"{Colors.GREEN}Passed: {passed}{Colors.RESET}")
        print(f"{Colors.RED}Failed: {failed}{Colors.RESET}")
        return passed, failed
        
    except Exception as e:
        print(f"{Colors.RED}Error: {str(e)}{Colors.RESET}")
        return 0, 1


def main():
    """Run all tests"""
    print(f"{Colors.BOLD}{Colors.CYAN}")
    print("="*60)
    print("Medium Data/ML Coding - Automated Test Suite")
    print("="*60)
    print(f"{Colors.RESET}")
    
    exercise_num = None
    if len(sys.argv) > 1:
        if '--exercise' in sys.argv or '-e' in sys.argv:
            try:
                idx = sys.argv.index('--exercise') if '--exercise' in sys.argv else sys.argv.index('-e')
                exercise_num = int(sys.argv[idx + 1])
            except (IndexError, ValueError):
                print(f"{Colors.RED}Invalid exercise number. Use: python test_all.py --exercise <1-4>{Colors.RESET}")
                return
    
    total_passed = 0
    total_failed = 0
    
    tests = [
        (1, test_exercise_1),
        (2, test_exercise_2),
        (3, test_exercise_3),
        (4, test_exercise_4),
    ]
    
    if exercise_num:
        tests = [(num, func) for num, func in tests if num == exercise_num]
        if not tests:
            print(f"{Colors.RED}Exercise {exercise_num} not found.{Colors.RESET}")
            return
    
    for num, test_func in tests:
        passed, failed = test_func()
        total_passed += passed
        total_failed += failed
    
    print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*60}")
    print("Final Summary")
    print(f"{'='*60}{Colors.RESET}")
    print(f"{Colors.GREEN}Total Passed: {total_passed}{Colors.RESET}")
    print(f"{Colors.RED}Total Failed: {total_failed}{Colors.RESET}")
    
    if total_failed == 0:
        print(f"{Colors.GREEN}{Colors.BOLD}All tests passed! ðŸŽ‰{Colors.RESET}")
        return 0
    else:
        print(f"{Colors.YELLOW}Some tests failed. Keep working! ðŸ’ª{Colors.RESET}")
        return 1


if __name__ == "__main__":
    sys.exit(main())

